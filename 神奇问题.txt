Content-Type: text/x-zim-wiki
Wiki-Format: zim 0.4
Creation-Date: 2015-10-27T21:38:36+08:00

====== 神奇问题 ======
Created 星期二 27 十月 2015

* ==== 本地某个电脑（ubuntu)突然不能访问40的服务 ====

	现象：
		也不能ssh或者telnet或者随意开的一个socket接口
		可以ping通
		其它同属于一个nat下的电脑大部分可以访问，个别电脑也不能访问
		过一断时候又可以正常访问或者连接
		无特定场景可以复现问题
	解决步骤：
	1. 在服务端抓包，发现服务端收到了建立tcp连接的请求，但不做任何回应，导致tcp连接无法建立
	2. 对比可以正常回应的数据包和不能正常回应的数据包，发现二层,ip层数据包均没有本质区别，考虑到可以ping通说，说明ip层本身也没有问题
	3. 对比tcp层数据包，发现在window size和options.timestamps TSval上有所不同，于是查找这两者所起的作用
	4. http://stackoverflow.com/questions/8893888/dropping-of-connections-asteriskwith-tcp-tw-recycle
		a. 这篇提到，如果tcp_tw_recycle启用，那么在nat后面的主机的timestamp在服务端是混乱的，会导致服务器的recycle机制出错，一些主机无法连接服务器 
		b. 而出问题服务器的tcp_tw_reuse, tcp_tw_recycle, tcp_timestamps 都处于启用状态
	5. http://serverfault.com/questions/664655/linux-clients-cant-connect-server-and-tcp-windows-size-timestamps-issue 这篇提到：
			a.Both users are behind the same NAT device (in our case, a Fortigate firewall). This firewall will assign local ports on it's external interface/IP to each NAT'ed connection. If the port is already in use to another user, it is skipped. If the connection is closed, the port is released and returned to the NAT pool. If that port is then assigned to the other user, but the server still has some record of the connection (TIME_WAIT, final FIN/ACK not received) and the timestamp of the packet is lower of that of the previous connection, the packet will be silently disgarded.
		6.最后解决办法：
			tcp_tw_reuse = 1, tcp_timestamps = 1 timestamp的使用和tw_use的使用将确保tcp端口复用冲突（这需要客户端也启用timestamps, 这个默认都是启用）
			但如果再开启tcp_tw_recycle, 就会出现冲突so: tcp_tw_recycle=0
			(net.ipv4.tcp_window_scaling 有说法这个也要关闭，但是会引动性能问题，从原理上讲这个应该是不需要关闭的）
			建议使用更短更合适的tcp_timewait_len
	7. 综上：
		a. tcp_tw_reuse = 1
		b. tcp_timestamps = 1
		c. tcp_tw_recycle=0
		d. tcp_timewait_len 可以小一些

* =========== 使用mysqldump 备份数据库时，报错runtime/cgo: pthread_create failed: Resource temporarily unavailable ===================
	1. 感觉应该是允许的线程数量用尽了
	2. 更改ulimit -u (max user processes), 就好了，可是这说的不是进程数么?! 但是经查实max user processes 说的是线程数，fuck me!!!!!!!!
	其它问题：http://www.tuicool.com/articles/73E7F3   关键就是: 系统调用阻塞时大量生成内核级线程导致的 , 而cgo启用的情况下, 每一次DNS查询,都会起动一个系统线程! 于是卡的时候资源耗尽
	
	

* ================= 37, 38, 39 单个cpu温度过高的问题 ==================
37，38，39单物理核心温度过高问题暂时规避

问题现象：

机器的第一个物理核心温度常年逼近critical 值90，而系统的cpu load只在10左右。38、39的多次自动重启怀疑与此有关。

问题定位：

1）从netdata上看，系统负载绝大部分分配到了物理核心1上。研究发现系统cpu负载调度有如下机制：

	cpu核心间的负载均衡优先在同一个逻辑核心的不同超线程上完成，其次是同一个物理核心的不同逻辑核，最后才是不同的物理核心。这么处理是尽可能让程序在相同物理核心上运行，可以共享二级缓存、共享内存，提高运行速度的同时还可以省电(因为其它物理核负载低，可以降频甚至关闭)。关键词(sched_domain, 内核代码: kernel/sched)

2) 按照1的思路尝试调整系统参数以影响cpu核心调度机制。sysctl -a | grep sched_domain可以看到有很多相关参数，尝试调整flags\imbalance_pct\busy_factor等均不起作用。且设置完这些参数后过一段时间，这些参数又回到默认值。证明设置无效。同时没有找到更多的sched_domain资料，此路不通。

3) 再分析这些机器的特殊性——网络负载高。对比35nlp机器，并没有类似的负载不均的问题。因此查看机器的网络中断，发现机器的网络中断也有类似问题，即中断均分配到物理核心1上。

4) 按照3的思路，尝试调整网络中断使其分配到不同的cpu上。

	理论上irqbalance这个系统服务应该已经处理好了irq的分配 ，然而查看后发现，irqbalance并不如想像中起了作用，网络中断固定分配到了物理核心1应该也是如同cpu负载均衡一样的原理，为了快和省电而有某些神奇怪的机制http://blog.csdn.net/whrszzc/article/details/50533866，而且一担大部分中断分配到同一个物理核心后，无法再自动分配到其它物理核心，除非改irqbalance的代码重新编译。

5) 继续4的思路，尝试配置irqbalance使其跳过1号cpu, 发现如果完全禁掉物理cpu1, irqbalance还是不会把中断分配到其它物理CPU，而是全部指到CPU1的第一个逻辑核心，效果更糟。

另外又尝试了修改/proc/irq/$irq/affinity_hint, 文档中说这里可以强制指定。然后尝试后发现不能修改该文件，简单查之后发现可能与centos系统有关。

 最后无耐关闭irqbalance，手动将网络中断分配到非物理核心1。发现好使！



最后规避(因为还是闹不清原理)办法：

1） 关闭irqbalance

2)   系统启动时，将em1网络中断强制分配到2,3,4号U，暴力脚本如下：


# adjust em1 irq affinity


em1_irqs=`grep em1 /proc/interrupts  |awk -F ":" '{print $1}'`

masks=(2 4 8 20 40 80 200 400 800)

len=${#masks[@]}

i=0

for irq in $em1_irqs; do

  let mi=i%len

  m=${masks[$mi]}

  echo "$i:$irq:$mi:$m"

  echo $m > /proc/irq/$irq/smp_affinity

  let i=i+1

done


PS: 此招可以使得cpu温度均在80以下

* ========================= 2021-05-14 有些MAC的CURL以及Chrome无法访问代理https服务的问题 =====================

现象：
1. 有些MAC可以有些不行
2. 在同一个电脑起的代理服务，通过127.0.0.1可以正常访问，但通过公司网络DHCP分配的IP就不行
3. 是在某一个时间点之后突然出现这个现象，在那个时间点之前，一切正常使用
4. 不行的电脑，使用ROOT运行CURL测试又是可以的

问题定位：
1. 使用wireshark抓包，逐字节比对成功时与不成功时的网络请求，发现网络请求完全没有区别。
   失败的时候卡在应用程序根据代理协议发起HTTP CONNECT的时候，服务端有正常的200返回，抓包也能看到该200返回的TCP ACK。
   但应用程序就是没有正常发起后续的Client Hello请求。

2. 重新编译CURL，在相关位置打印日志，发现程序卡在系统调用select上，socket一直轮询不到内容。

3. 另一个发现的现象是，CURL 一直在轮询，到2分钟后服务端主动关闭TCP连接时，就可以轮询到HTTP 200返回了。但此时因为超时太久，CURL判断TLS握手失败。

4. 根据1，2，3判断，程序与服务端CONNECT的这一下，收发包是正常的，惟一的问题出在系统收到HTTP 200返回后，没有及时提交给应用程序（表现为select不到内容）。

5. 为验证4的判断，做了两个尝试：
    1）使用node express服务直接起一个HTTP服务，返回和CONNECT正常返回一模一样的内容，测试时发现，express返回相应内容时，会加上额外的一些字节，如下：
    ```
    HTTP/1.1 200 OK
    Date: Thu, 13 May 2021 09:46:14 GMT
    Transfer-Encoding: chunked

    0     ----------> 这个是比代理服务多返回的值
    ```
    经查发现，这个多返回的0，与HTTP/1.1新头部 Transfer-Encoding: chunked有关，这个0表示HTTP分片结束。

    那这么看起来代理服务返回的HTTP 200数据包确实是“不完善”的，至少没有完全支持HTTP/1.1的协议

    2) 根据第一步的发现，进一步测试，直接使用python起一个socket服务，直接返回HTTP 200文本内容

    并测试在有没有HTTP分片标识、有没有Transfer-Encoding: chunked头部的条件下，CURL能否及时收到包并发起后续的Client Hello请求

    结论：在有正常的HTTP分片结束标识、或者没有Transfer-Encoding: chunked头部的时候，CURL均可以正常发起后续请求；
         而如果返回和代理服务一模一样的结果(没有0结束标志)，同样会出现CURL select不到内容的情况；

6. 根据5的尝试，问题解决方案就是调整代理服务，返回完整的分片结束报文、或者去掉Transfer-Encoding: chunked头部

7. 现象解释：
   1）为什么有的电脑行有的电脑不行，且在某个时间点之后突然出现这个现象，且通过127.0.0.1访问总是没有问题？
   答：猜测：可能跟IT的一些防火墙策略或者系统优化策略有关，比如系统是否有一些上层协议优化选项，在socket收发时做高层协议判断，发现是HTTP分片是，主动等待其它分片的到来，
   而不是无脑提交给应用层去处理。而行的时候，可能是此类优化选项没开启，比如ROOT状态、另一些电脑等

   2）为啥在代理服务这程序之间，再挂一层charles就又可以了？
   HTTP CONNECT是赂charles发起的，charles返回的HTTP 200经抓包发现直接没有Transfer-Encoding: chunked这个头部，根据上面的分析自然是可以的

   3) 为啥http可以，https不行？
   http代理没有CONNECT这一下
