* curd的报错：UnexpectedError: must be str, not bytes, 其实是phoenix返回了408(没有在服务端规定的时间内完成请求发送), 是解析这个错误时的报错：
b"<html><body><h1>408 Request Time-out</h1>\nYour browser didn't send a complete request in time.
\n</body></html>\n

* phoenix-sqlline 显示长度设置：!set maxWidth 2048
* 查看hbase table的split points:
    hbase(main):011:0* sfg=get_table 'TAOBAO_COMMENT_SPLIT';
    hbase(main):012:0* sfg.get_splits()
* phoenix-sqlline.py安装：直接下载压缩包解压，里面的bin/*可以直接运行
* phoenixdb cursor放久后execute得到空结果的问题：phoenixdb.cursor.py里execute时，如果参数params是None, 为被当成CreateStatement处理，
  statement_id维持上一个(同一个cursor)，猜测相同的statement_id在后端不是全新的查询，返回空可能与不同statement_id没变有关

* 命令行连接hbase:
    1. 下载phoenix解压进到bin目录执行sqlline.py
    2. 需要各个hbase服务器的主机名解析正确
    3. /etc/hbase/conf/hbase-site.xml里需要加配置项(如果没有该文件及目录，可从phoenix解压后文件中找出并复制到相应位置再修改)：
        ```
        <property>
            <name>phoenix.schema.isNamespaceMappingEnabled</name>
            <value>true</value>
        </property>
        ```

* squirrel(图形客户端)连接hbase:
    1. 下载安装squirrel, 下载phoenix并解压
    2. 由于我们的hbase开启了namespaceMapping，而squirrel无法进行相应配置。
       尝试用`jar uf phoenix-4.14.0-cdh5.14.2-client.jar /etc/hbase/conf/hbase-site.xml`将配置文件压到client jar里也不行
    3. 网上找的方法，需要使用瘦连接的方式，我们的hbase本身也起了瘦服务端: `https://blog.csdn.net/wumiqing1/article/details/85088777`
       - 将瘦客户端jar拷贝到squirrel java/lib目录： phoenix-4.14.0-cdh5.14.2-thin-client.jar
       - 按官方说明`http://phoenix.apache.org/installation.html`配置phoenix driver时
           - URI: `jdbc:phoenix:thin:url=http://10.3.10.233:8765;serialization=PROTOBUF`
           - 类名: `org.apache.phoenix.queryserver.client.Driver`
       - alias链接url同样：`jdbc:phoenix:thin:url=http://10.3.10.233:8765;serialization=PROTOBUF`

* phoenix client.jar 读不到nameMappingEnable配置的问题：
    - HADOOP_CLASSPATH="$(hbase mapredcp):/etc/hbase/conf" hadoop jar /opt/cloudera/parcels/APACHE_PHOENIX/lib/phoenix/phoenix-4.14.0-cdh5.14.2-client.jar org.apache.phoenix.mapreduce.CsvBulkLoadTool --table TMALL_COMMENT_MR --input /sfg/tmall.comment.-8975801478601287988_-8975491383539225521.csv
    - HADOOP_CLASSPATH="/opt/cloudera/parcels/CDH-5.14.2-1.cdh5.14.2.p0.3/jars/hbase-protocol-1.2.0-cdh5.14.2.jar:/etc/hbase/conf" hadoop jar /opt/cloudera/parcels/APACHE_PHOENIX/lib/phoenix/phoenix-4.14.0-cdh5.14.2-client.jar org.apache.phoenix.mapreduce.CsvBulkLoadTool --table TMALL_COMMENT_MR --input /sfg/sfg.csv -c ID,COMMENT_TIME,CONTENT,ORIGIN_DATA,PRODUCT_ID,SKU_ID,SNAPSHOT_TIME,USER_LEVEL_ID,USER_NICKNAME
    - 这个环境变量可能也有用：HBASE_CONF_DIR=/etc/hbase/conf/
